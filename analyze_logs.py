#!/usr/bin/env python3
"""
Log analyzer for mem0 + Ollama integration

This script parses and analyzes the log files generated by the application,
providing insights into errors, performance, and usage patterns.
"""

import os
import re
import sys
import json
import argparse
import datetime
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple, Optional, Set

# Default log directory
LOG_DIR = os.path.join(os.getcwd(), "logs")

# Log files
MAIN_LOG = os.path.join(LOG_DIR, "main.log")
ERROR_LOG = os.path.join(LOG_DIR, "error.log")
API_LOG = os.path.join(LOG_DIR, "api_calls.log")
MEMORY_LOG = os.path.join(LOG_DIR, "memory_ops.log")

# Regular expressions for parsing logs
LOG_PATTERN = re.compile(
    r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \| '
    r'(?P<level>\w+)\s+ \| '
    r'(?P<module>[^\|]+) \| '
    r'(?P<request_id>[^\|]+) \| '
    r'(?P<message>.*)'
)

ERROR_PATTERN = re.compile(r'Error|Exception|Failed|Timeout|Failure', re.IGNORECASE)
WARNING_PATTERN = re.compile(r'Warning|Warn|Could not', re.IGNORECASE)

class LogAnalyzer:
    """Analyze log files for errors, warnings, and performance metrics."""
    
    def __init__(self, log_dir: str = LOG_DIR, days: int = 1):
        """
        Initialize the log analyzer.
        
        Args:
            log_dir: Directory containing the log files
            days: Number of days to analyze
        """
        self.log_dir = log_dir
        self.days = days
        self.cutoff_time = self._get_cutoff_time(days)
        
        # Results data structures
        self.error_counts = Counter()
        self.warning_counts = Counter()
        self.api_metrics = defaultdict(list)
        self.memory_metrics = defaultdict(list)
        self.request_ids = set()
        self.error_contexts = defaultdict(list)
        
        # Categorized errors by module and source
        self.errors_by_module = defaultdict(Counter)
        self.errors_by_operation = defaultdict(Counter)
        
        # Performance metrics
        self.operation_durations = defaultdict(list)
        
        # API metrics
        self.api_status_codes = Counter()
        self.api_endpoints = Counter()
        
        # Overall stats
        self.total_logs = 0
        self.error_logs = 0
        self.warning_logs = 0
        self.api_calls = 0
        self.memory_operations = 0
    
    def _get_cutoff_time(self, days: int) -> Optional[datetime.datetime]:
        """Calculate the cutoff time for log analysis."""
        if days <= 0:
            return None  # No cutoff - analyze all logs
        
        now = datetime.datetime.now()
        return now - datetime.datetime.timedelta(days=days)
    
    def _convert_timestamp(self, timestamp_str: str) -> datetime.datetime:
        """Convert a timestamp string to a datetime object."""
        return datetime.datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f')
    
    def _should_process_log(self, timestamp_str: str) -> bool:
        """Check if a log entry should be processed based on the cutoff time."""
        if self.cutoff_time is None:
            return True
        
        log_time = self._convert_timestamp(timestamp_str)
        return log_time >= self.cutoff_time
    
    def _extract_error_context(self, lines: List[str], error_line_idx: int) -> List[str]:
        """Extract context lines around an error."""
        start_idx = max(0, error_line_idx - 3)
        end_idx = min(len(lines), error_line_idx + 4)
        return lines[start_idx:end_idx]
    
    def _categorize_error(self, module: str, message: str) -> str:
        """Categorize an error message for better grouping."""
        # Extract the error type
        if "Exception:" in message:
            parts = message.split("Exception:", 1)
            error_type = parts[0].strip().split()[-1]
            return f"{error_type}Exception"
        
        # Look for common error patterns
        error_patterns = [
            (r'Timeout', 'TimeoutError'),
            (r'Connection refused', 'ConnectionRefusedError'),
            (r'Failed to connect', 'ConnectionError'),
            (r'No such file', 'FileNotFoundError'),
            (r'Permission denied', 'PermissionError'),
            (r'Invalid JSON', 'JsonDecodeError'),
            (r'API call \[.*\] failed', 'ApiCallError'),
            (r'Error initializing', 'InitializationError')
        ]
        
        for pattern, category in error_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return category
        
        # Default to a generic category
        return "OtherError"
    
    def analyze_main_log(self):
        """Analyze the main log file."""
        if not os.path.exists(MAIN_LOG):
            print(f"Warning: Main log file not found at {MAIN_LOG}")
            return
        
        with open(MAIN_LOG, 'r') as f:
            lines = f.readlines()
            
        for i, line in enumerate(lines):
            self.total_logs += 1
            match = LOG_PATTERN.match(line)
            if not match:
                continue
                
            timestamp = match.group('timestamp')
            if not self._should_process_log(timestamp):
                continue
                
            level = match.group('level')
            module = match.group('module').strip()
            request_id = match.group('request_id').strip()
            message = match.group('message')
            
            if request_id != "-":
                self.request_ids.add(request_id)
            
            # Process by log level
            if level == "ERROR":
                self.error_logs += 1
                error_category = self._categorize_error(module, message)
                self.error_counts[error_category] += 1
                self.errors_by_module[module][error_category] += 1
                
                # Extract context for this error
                context = self._extract_error_context(lines, i)
                self.error_contexts[error_category].append(context)
            
            elif level == "WARNING":
                self.warning_logs += 1
                self.warning_counts[message.split(':')[0]] += 1
                
            # Extract operation durations
            duration_match = re.search(r'(\w+) in (\d+\.\d+)ms', message)
            if duration_match:
                operation = duration_match.group(1)
                duration = float(duration_match.group(2))
                self.operation_durations[operation].append(duration)
                
                if operation == "chat_with_ollama":
                    self.api_metrics['chat_duration'].append(duration)
                elif operation.startswith("add_") or operation == "search":
                    self.memory_metrics[operation].append(duration)
    
    def analyze_error_log(self):
        """Analyze the dedicated error log file."""
        if not os.path.exists(ERROR_LOG):
            print(f"Warning: Error log file not found at {ERROR_LOG}")
            return
        
        with open(ERROR_LOG, 'r') as f:
            content = f.read()
            
        # Analyze stack traces
        stack_traces = content.split("Stack trace:")
        for trace in stack_traces[1:]:  # Skip the first split which is before any stack trace
            lines = trace.strip().split('\n')
            if not lines:
                continue
                
            # Find the main error line
            error_line = next((line for line in lines if "Error" in line or "Exception" in line), "Unknown error")
            
            # Categorize the error
            error_category = self._categorize_error("", error_line)
            
            # Find the source file/operation
            source_line = next((line for line in lines if "/" in line and ".py" in line), "Unknown source")
            operation_match = re.search(r'/([^/]+)\.py', source_line)
            operation = operation_match.group(1) if operation_match else "unknown"
            
            # Count this error
            self.errors_by_operation[operation][error_category] += 1
    
    def analyze_api_log(self):
        """Analyze the API calls log file."""
        if not os.path.exists(API_LOG):
            print(f"Warning: API log file not found at {API_LOG}")
            return
        
        with open(API_LOG, 'r') as f:
            lines = f.readlines()
            
        current_call = {}
        
        for line in lines:
            match = LOG_PATTERN.match(line)
            if not match:
                continue
                
            timestamp = match.group('timestamp')
            if not self._should_process_log(timestamp):
                continue
                
            message = match.group('message')
            request_id = match.group('request_id').strip()
            
            # New API call
            api_call_match = re.search(r'API Call \[([^\]]+)\]: (\w+) ([^\s]+)', message)
            if api_call_match:
                self.api_calls += 1
                call_id = api_call_match.group(1)
                method = api_call_match.group(2)
                url = api_call_match.group(3)
                
                # Extract endpoint
                endpoint = url.split('/')[-1] if '/' in url else url
                self.api_endpoints[endpoint] += 1
                
                current_call = {
                    'id': call_id,
                    'method': method,
                    'url': url,
                    'endpoint': endpoint
                }
                
            # API response
            response_match = re.search(r'API Response \[([^\]]+)\]: Status: (\d+)', message)
            if response_match and current_call:
                call_id = response_match.group(1)
                status_code = int(response_match.group(2))
                
                # Only process if it matches the current call
                if call_id == current_call.get('id'):
                    self.api_status_codes[status_code] += 1
                    
                    # Extract duration if available
                    duration_match = re.search(r'Duration: (\d+\.\d+)ms', message)
                    if duration_match:
                        duration = float(duration_match.group(1))
                        endpoint = current_call.get('endpoint', 'unknown')
                        self.api_metrics[f"{endpoint}_duration"].append(duration)
                        
                    # Reset current call
                    current_call = {}
    
    def analyze_memory_log(self):
        """Analyze the memory operations log file."""
        if not os.path.exists(MEMORY_LOG):
            print(f"Warning: Memory log file not found at {MEMORY_LOG}")
            return
        
        with open(MEMORY_LOG, 'r') as f:
            lines = f.readlines()
            
        for line in lines:
            match = LOG_PATTERN.match(line)
            if not match:
                continue
                
            timestamp = match.group('timestamp')
            if not self._should_process_log(timestamp):
                continue
                
            message = match.group('message')
            
            # Memory operation
            memory_op_match = re.search(r'Memory (\w+) \[(\w+)\]', message)
            if memory_op_match:
                self.memory_operations += 1
                operation = memory_op_match.group(1)
                status = memory_op_match.group(2)
                
                # Track success/failure
                if status == "SUCCESS":
                    # Extract duration if available
                    duration_match = re.search(r'Duration: (\d+\.\d+)ms', message)
                    if duration_match:
                        duration = float(duration_match.group(1))
                        self.memory_metrics[f"{operation.lower()}_duration"].append(duration)
    
    def analyze_all(self):
        """Analyze all log files."""
        print(f"Analyzing logs from the past {self.days} days...")
        
        self.analyze_main_log()
        self.analyze_error_log()
        self.analyze_api_log()
        self.analyze_memory_log()
        
        print(f"Analysis complete. Found {self.error_logs} errors and {self.warning_logs} warnings.")
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate a comprehensive report of the log analysis."""
        report = {
            "period": f"Last {self.days} days" if self.days > 0 else "All time",
            "generated_at": datetime.datetime.now().isoformat(),
            "summary": {
                "total_logs": self.total_logs,
                "error_logs": self.error_logs,
                "warning_logs": self.warning_logs,
                "api_calls": self.api_calls,
                "memory_operations": self.memory_operations,
                "unique_request_ids": len(self.request_ids)
            },
            "errors": {
                "by_type": dict(self.error_counts.most_common(10)),
                "by_module": {
                    module: dict(counts.most_common(5))
                    for module, counts in self.errors_by_module.items()
                },
                "by_operation": {
                    op: dict(counts.most_common(5))
                    for op, counts in self.errors_by_operation.items()
                }
            },
            "warnings": dict(self.warning_counts.most_common(10)),
            "performance": {
                "operations": {
                    op: {
                        "count": len(durations),
                        "avg_ms": sum(durations) / len(durations) if durations else 0,
                        "min_ms": min(durations) if durations else 0,
                        "max_ms": max(durations) if durations else 0
                    }
                    for op, durations in self.operation_durations.items()
                },
                "api": {
                    endpoint: {
                        "count": self.api_endpoints.get(endpoint, 0),
                        "avg_ms": sum(self.api_metrics.get(f"{endpoint}_duration", [0])) / len(self.api_metrics.get(f"{endpoint}_duration", [1])) if self.api_metrics.get(f"{endpoint}_duration") else 0
                    }
                    for endpoint in set(self.api_endpoints.keys())
                },
                "memory": {
                    op: {
                        "avg_ms": sum(durations) / len(durations) if durations else 0,
                        "count": len(durations)
                    }
                    for op, durations in self.memory_metrics.items() if durations
                }
            },
            "api": {
                "endpoints": dict(self.api_endpoints.most_common()),
                "status_codes": {str(code): count for code, count in self.api_status_codes.items()}
            }
        }
        
        return report
    
    def print_summary(self):
        """Print a summary of the log analysis to the console."""
        report = self.generate_report()
        
        print("\n" + "="*50)
        print(f"LOG ANALYSIS REPORT: {report['period']}")
        print("="*50)
        
        print(f"\nGenerated at: {report['generated_at']}")
        print("\nSUMMARY:")
        for key, value in report['summary'].items():
            print(f"  - {key.replace('_', ' ').title()}: {value}")
        
        print("\nTOP ERRORS:")
        if report['errors']['by_type']:
            for error, count in report['errors']['by_type'].items():
                print(f"  - {error}: {count}")
        else:
            print("  No errors found")
        
        print("\nPERFORMANCE METRICS:")
        print("  API Call Durations:")
        for op, metrics in report['performance']['api'].items():
            if metrics['count'] > 0:
                print(f"  - {op}: {metrics['avg_ms']:.2f}ms avg ({metrics['count']} calls)")
        
        print("  Memory Operation Durations:")
        for op, metrics in report['performance']['memory'].items():
            if 'duration' in op and metrics['count'] > 0:
                print(f"  - {op.replace('_duration', '')}: {metrics['avg_ms']:.2f}ms avg ({metrics['count']} ops)")
        
        print("\nAPI STATUS CODES:")
        for code, count in report['api']['status_codes'].items():
            print(f"  - {code}: {count}")
        
        print("\nRECOMMENDATIONS:")
        self._print_recommendations(report)
        
        print("\n" + "="*50)
        print("End of Report")
        print("="*50 + "\n")
    
    def _print_recommendations(self, report: Dict[str, Any]):
        """Generate and print recommendations based on the analysis."""
        recommendations = []
        
        # Check for high error rates
        error_rate = report['summary']['error_logs'] / report['summary']['total_logs'] if report['summary']['total_logs'] > 0 else 0
        if error_rate > 0.05:  # More than 5% errors
            recommendations.append(
                f"High error rate detected ({error_rate:.1%}). Review most common errors and implement fixes."
            )
        
        # Look for slow API endpoints
        slow_endpoints = []
        for endpoint, metrics in report['performance']['api'].items():
            if metrics['avg_ms'] > 1000:  # Slower than 1 second
                slow_endpoints.append((endpoint, metrics['avg_ms']))
        
        if slow_endpoints:
            slow_list = ", ".join(f"{ep} ({ms:.0f}ms)" for ep, ms in slow_endpoints)
            recommendations.append(
                f"Slow API endpoints detected: {slow_list}. Consider optimization."
            )
        
        # Check for API errors (4xx, 5xx status codes)
        api_errors = sum(count for code, count in report['api']['status_codes'].items() 
                        if code.startswith('4') or code.startswith('5'))
        if api_errors > 0:
            recommendations.append(
                f"Found {api_errors} API calls with error status codes. Review API error handling."
            )
        
        # Memory operation recommendations
        slow_memory_ops = []
        for op, metrics in report['performance']['memory'].items():
            if 'duration' in op and metrics['avg_ms'] > 500:  # Slower than 500ms
                slow_memory_ops.append((op.replace('_duration', ''), metrics['avg_ms']))
        
        if slow_memory_ops:
            slow_list = ", ".join(f"{op} ({ms:.0f}ms)" for op, ms in slow_memory_ops)
            recommendations.append(
                f"Slow memory operations detected: {slow_list}. Consider optimization."
            )
        
        # Print recommendations
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        else:
            print("  No specific issues detected. System appears to be running normally.")
    
    def save_report(self, output_file: str = "log_analysis.json"):
        """Save the report to a JSON file."""
        report = self.generate_report()
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
            
        print(f"\nDetailed report saved to {output_file}")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Analyze mem0 + Ollama integration logs")
    parser.add_argument("--days", type=int, default=1, help="Number of days to analyze")
    parser.add_argument("--log-dir", type=str, default=LOG_DIR, help="Directory containing log files")
    parser.add_argument("--output", type=str, default="log_analysis.json", help="Output file for detailed report")
    parser.add_argument("--summary-only", action="store_true", help="Only print summary, don't save report")
    
    args = parser.parse_args()
    
    analyzer = LogAnalyzer(log_dir=args.log_dir, days=args.days)
    analyzer.analyze_all()
    analyzer.print_summary()
    
    if not args.summary_only:
        analyzer.save_report(args.output)

if __name__ == "__main__":
    main()